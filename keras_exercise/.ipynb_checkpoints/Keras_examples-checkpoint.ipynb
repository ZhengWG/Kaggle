{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras应用实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考自 [keras_team](https://github.com/keras-team/keras/tree/master/examples)\n",
    "\n",
    "实例包括：图像（视频），文字（序列），生成模型，Keras特色功能实现，这里先简单探讨图像和Keras模型功能部分。\n",
    "\n",
    "软件环境：\n",
    "python3.5 tensorflow1.12.0 keras.2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minist_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 0.2419 - acc: 0.9260 - val_loss: 0.1378 - val_acc: 0.9563\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1039 - acc: 0.9698 - val_loss: 0.0789 - val_acc: 0.9759\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.0743 - acc: 0.9776 - val_loss: 0.0839 - val_acc: 0.9754\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0597 - acc: 0.9820 - val_loss: 0.0776 - val_acc: 0.9797\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0494 - acc: 0.9851 - val_loss: 0.0889 - val_acc: 0.9793\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0437 - acc: 0.9867 - val_loss: 0.0748 - val_acc: 0.9818\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.0399 - acc: 0.9882 - val_loss: 0.0847 - val_acc: 0.9804\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0333 - acc: 0.9901 - val_loss: 0.0757 - val_acc: 0.9832\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.0312 - acc: 0.9907 - val_loss: 0.0847 - val_acc: 0.9825\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0283 - acc: 0.9919 - val_loss: 0.0814 - val_acc: 0.9836\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0253 - acc: 0.9927 - val_loss: 0.0938 - val_acc: 0.9815\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0255 - acc: 0.9926 - val_loss: 0.0906 - val_acc: 0.9821\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.0211 - acc: 0.9935 - val_loss: 0.1134 - val_acc: 0.9804\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0224 - acc: 0.9937 - val_loss: 0.0971 - val_acc: 0.9839\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.0237 - acc: 0.9938 - val_loss: 0.0961 - val_acc: 0.9819\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0192 - acc: 0.9944 - val_loss: 0.1032 - val_acc: 0.9840\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.0202 - acc: 0.9944 - val_loss: 0.1047 - val_acc: 0.9832\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.0180 - acc: 0.9955 - val_loss: 0.1054 - val_acc: 0.9816\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0192 - acc: 0.9951 - val_loss: 0.1253 - val_acc: 0.9819\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0189 - acc: 0.9952 - val_loss: 0.1063 - val_acc: 0.9821\n",
      "Test loss: 0.10632506377054074\n",
      "Test accuracy: 0.9821\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Trains a simple deep NN on the MNIST dataset.\n",
    "20 epochs\n",
    "acc: 0.9816\n",
    "Test loss: 0.11610001068654767\n",
    "Test accuracy: 0.9816\n",
    "about 6 seconds per epoch on a GeForce 940M GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# 将数据集分割为训练集和验证集\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# 类型标签需要转化为keras要求的binary class matrics格式\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 小图片分类示例（Sequential式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 122s 2ms/step - loss: 1.8311 - acc: 0.3345 - val_loss: 1.6657 - val_acc: 0.4035\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 1.5049 - acc: 0.4567 - val_loss: 1.3474 - val_acc: 0.5146\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 1.3528 - acc: 0.5153 - val_loss: 1.2531 - val_acc: 0.5597\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 1.2483 - acc: 0.5585 - val_loss: 1.1387 - val_acc: 0.5976\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 103s 2ms/step - loss: 1.1618 - acc: 0.5917 - val_loss: 1.0871 - val_acc: 0.6129\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 115s 2ms/step - loss: 1.0942 - acc: 0.6180 - val_loss: 1.0086 - val_acc: 0.6451\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 103s 2ms/step - loss: 1.0383 - acc: 0.6376 - val_loss: 1.0126 - val_acc: 0.6431\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 104s 2ms/step - loss: 0.9970 - acc: 0.6500 - val_loss: 0.9596 - val_acc: 0.6581\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 109s 2ms/step - loss: 0.9558 - acc: 0.6686 - val_loss: 0.8889 - val_acc: 0.6854\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.9255 - acc: 0.6781 - val_loss: 0.8605 - val_acc: 0.6976\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 111s 2ms/step - loss: 0.8998 - acc: 0.6863 - val_loss: 0.8711 - val_acc: 0.6937\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 0.8745 - acc: 0.6970 - val_loss: 0.8339 - val_acc: 0.7066\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 109s 2ms/step - loss: 0.8500 - acc: 0.7048 - val_loss: 0.8314 - val_acc: 0.7133\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 115s 2ms/step - loss: 0.8319 - acc: 0.7119 - val_loss: 0.8137 - val_acc: 0.7161\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 0.8099 - acc: 0.7187 - val_loss: 0.7881 - val_acc: 0.7241\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 0.7941 - acc: 0.7254 - val_loss: 0.8105 - val_acc: 0.7192\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 76s 2ms/step - loss: 0.7766 - acc: 0.7324 - val_loss: 0.7911 - val_acc: 0.7241\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 76s 2ms/step - loss: 0.7653 - acc: 0.7361 - val_loss: 0.7359 - val_acc: 0.7421\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 76s 2ms/step - loss: 0.7556 - acc: 0.7377 - val_loss: 0.7487 - val_acc: 0.7437\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 75s 2ms/step - loss: 0.7440 - acc: 0.7440 - val_loss: 0.7302 - val_acc: 0.7474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8cd7b1b438>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Trains a simple deep NN on the CIFAR10 dataset.\n",
    "20 epochs\n",
    "acc(data auged): 0.6818(train),0.7230(val)\n",
    "Test loss: 0.8004208864212036\n",
    "Test accuracy: 0.723\n",
    "about 77 seconds per epoch on a GeForce 940M GPU.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "# data_augmentation = True\n",
    "# num_predictions = 20\n",
    "\n",
    "# 数据载入\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 多分类标签生成\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# 网络结构配置\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# 训练参数设置\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 生成训练数据\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# 无数据增强进行训练\n",
    "print('Not using data augmentation.')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/20\n",
      "1562/1562 [==============================] - 68s 44ms/step - loss: 1.8172 - acc: 0.3348 - val_loss: 1.5825 - val_acc: 0.4202\n",
      "Epoch 2/20\n",
      "1562/1562 [==============================] - 68s 43ms/step - loss: 1.5852 - acc: 0.4221 - val_loss: 1.4322 - val_acc: 0.4794\n",
      "Epoch 3/20\n",
      "1562/1562 [==============================] - 72s 46ms/step - loss: 1.4609 - acc: 0.4720 - val_loss: 1.3075 - val_acc: 0.5254\n",
      "Epoch 4/20\n",
      "1562/1562 [==============================] - 73s 47ms/step - loss: 1.3757 - acc: 0.5065 - val_loss: 1.1862 - val_acc: 0.5771\n",
      "Epoch 5/20\n",
      "1562/1562 [==============================] - 74s 47ms/step - loss: 1.3029 - acc: 0.5371 - val_loss: 1.1373 - val_acc: 0.5927\n",
      "Epoch 6/20\n",
      "1562/1562 [==============================] - 78s 50ms/step - loss: 1.2501 - acc: 0.5535 - val_loss: 1.1786 - val_acc: 0.5838\n",
      "Epoch 7/20\n",
      "1562/1562 [==============================] - 78s 50ms/step - loss: 1.2071 - acc: 0.5704 - val_loss: 1.0379 - val_acc: 0.6281\n",
      "Epoch 8/20\n",
      "1562/1562 [==============================] - 78s 50ms/step - loss: 1.1705 - acc: 0.5858 - val_loss: 1.0924 - val_acc: 0.6180\n",
      "Epoch 9/20\n",
      "1562/1562 [==============================] - 78s 50ms/step - loss: 1.1352 - acc: 0.6012 - val_loss: 1.0783 - val_acc: 0.6285\n",
      "Epoch 10/20\n",
      "1562/1562 [==============================] - 78s 50ms/step - loss: 1.1033 - acc: 0.6117 - val_loss: 0.9947 - val_acc: 0.6515\n",
      "Epoch 11/20\n",
      "1562/1562 [==============================] - 78s 50ms/step - loss: 1.0768 - acc: 0.6206 - val_loss: 1.0336 - val_acc: 0.6323\n",
      "Epoch 12/20\n",
      "1562/1562 [==============================] - 77s 49ms/step - loss: 1.0511 - acc: 0.6332 - val_loss: 0.9094 - val_acc: 0.6791\n",
      "Epoch 13/20\n",
      "1562/1562 [==============================] - 78s 50ms/step - loss: 1.0251 - acc: 0.6381 - val_loss: 0.9096 - val_acc: 0.6755\n",
      "Epoch 14/20\n",
      "1562/1562 [==============================] - 78s 50ms/step - loss: 1.0056 - acc: 0.6471 - val_loss: 0.8570 - val_acc: 0.6995\n",
      "Epoch 15/20\n",
      "1562/1562 [==============================] - 77s 49ms/step - loss: 0.9874 - acc: 0.6542 - val_loss: 0.9028 - val_acc: 0.6851\n",
      "Epoch 16/20\n",
      "1562/1562 [==============================] - 77s 49ms/step - loss: 0.9740 - acc: 0.6592 - val_loss: 0.8249 - val_acc: 0.7121\n",
      "Epoch 17/20\n",
      "1562/1562 [==============================] - 77s 49ms/step - loss: 0.9581 - acc: 0.6642 - val_loss: 0.8329 - val_acc: 0.7088\n",
      "Epoch 18/20\n",
      "1562/1562 [==============================] - 76s 49ms/step - loss: 0.9416 - acc: 0.6722 - val_loss: 0.8716 - val_acc: 0.6998\n",
      "Epoch 19/20\n",
      "1562/1562 [==============================] - 77s 49ms/step - loss: 0.9323 - acc: 0.6758 - val_loss: 0.8601 - val_acc: 0.7030\n",
      "Epoch 20/20\n",
      "1562/1562 [==============================] - 77s 49ms/step - loss: 0.9186 - acc: 0.6818 - val_loss: 0.8004 - val_acc: 0.7230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf80eefc88>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "# 进行数据预处理和实时的数据增强:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # 是否控制输入数据集的均值为0\n",
    "    samplewise_center=False,  # 是否控制样本均值为0\n",
    "    featurewise_std_normalization=False,  # 全部输入是否除以数据集的标准偏差(std)\n",
    "    samplewise_std_normalization=False,  # 每个输入是否除以数据集的标准偏差\n",
    "    zca_whitening=False,  # 是否应用ZCA白化\n",
    "    rotation_range=0,  # 随机旋转(度数,0-180°)\n",
    "    width_shift_range=0.1,  # 随机水平平移(整个宽度比例)\n",
    "    height_shift_range=0.1,  # 随机竖直平移(整个高度比例)\n",
    "    horizontal_flip=True,  # 随机水平翻转\n",
    "    vertical_flip=False)  # 随机竖直翻转\n",
    "\n",
    "# 生成训练增强数据\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# fit训练\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at models/keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 4s 378us/step\n",
      "Test loss: 0.8004208864212036\n",
      "Test accuracy: 0.723\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "save_dir = 'models/'\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# 保存模型和权重\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# 测试训练模型\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet训练CIFAR10 小图片分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model         | n   | 200-epoch accuracy | Original paper accuracy | sec/epoch GTX1080Ti |\n",
    "|---------------|-----|--------------------|-------------------------|---------------------|\n",
    "| ResNet20 v1   | 3   | 92.16%             | 91.25%                  | 35                  |\n",
    "| Resnet32 v1   | 5   | 92.46%             | 92.49%                  | 50                  |\n",
    "| Resnet44 v1   | 7   | 92.50%             | 92.83%                  | 70                  |\n",
    "| Resnet56 v1   | 9   | 92.71%             | 93.03%                  | 90                  |\n",
    "| Resnet110 v1  | 18  | 92.65%             | 93.39+-.16%             | 165                 |\n",
    "| Resnet164 v1  | 27  | -                  | 94.07%                  | -                   |\n",
    "| Resnet1001 v1 | N/A | -                  | 92.39%                  | -                   |\n",
    "\n",
    "\n",
    "| Model         | n   | 200-epoch accuracy | Original paper accuracy | sec/epoch GTX1080Ti |\n",
    "|---------------|-----|--------------------|-------------------------|---------------------|\n",
    "| ResNet20 v2   | 2   | -%                 | -%                      | --                  |\n",
    "| Resnet32 v2   | N/A | NA%                | NA%                     | NA                  |\n",
    "| Resnet44 v2   | N/A | NA%                | NA%                     | NA                  |\n",
    "| Resnet56 v2   | 6   | 93.01%             | NA%                     | 100                 |\n",
    "| Resnet110 v2  | 12  | 93.15%             | 93.63%                  | 180                 |\n",
    "| Resnet164 v2  | 18  | -                  | 94.54%                  | -                   |\n",
    "| Resnet1001 v2 | 111 | -                  | 95.08+-.14%             | -                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# 训练参数\n",
    "batch_size = 16  # 原文中训练网络的batch_size为128, 官网为32\n",
    "epochs = 20\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "\n",
    "# 提取像素点均值以提高精度\n",
    "subtract_pixel_mean = True\n",
    "n = 3\n",
    "\n",
    "# Model version\n",
    "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "# ResNet模型的版本\n",
    "# 原始论文版本：version = 1 (ResNet v1), 改进的ResNet: version = 2 (ResNet v2)\n",
    "version = 1\n",
    "\n",
    "# 不同的版本网络深度不一样，与模型参数n有关\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2\n",
    "\n",
    "# 模型命名，深度，版本\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "# 加载CIFAR10数据\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 输入图片的维度\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# 标准化数据\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# 进行数据的均值操作(mean)\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# 将类型向量转化为二值类型矩阵（keras支持的格式）\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"\n",
    "    学习率规划:\n",
    "    学习率分别在80,120,160,180 epochs进行下降\n",
    "    在训练每个epoch后通过调用callbacks进行学习率的调整\n",
    "    \n",
    "    #参数\n",
    "        epoch(int):epochs数目\n",
    "    #返回值\n",
    "        lr(float):学习率\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"\n",
    "    定义基本的2D 卷积-BN-激活堆栈生成器(stack builder)\n",
    "    \n",
    "    #参数：\n",
    "        inputsr):从输入图片或者之前层中输入的张量(tensor)\n",
    "        num_filters(int):2维卷积，滤波器数目\n",
    "        kernel_size(int):2维卷积卷积核二维尺寸\n",
    "        activation(string):激活类型名称\n",
    "        batch_notmalization(bool):是否包含BN层\n",
    "        conv_first(bool):是否卷积层先，conv-bn-activation(True)或者bn-activation-conv(False)\n",
    "        \n",
    "    #返回值:\n",
    "        x(tensor):作为下一层的输入张量(tensor)\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"\n",
    "    ResNet V1 模型构建\n",
    "    \n",
    "    网络为最基本结构的堆栈组合(stacks):2x(3x3) Conv2D-BN-ReLu结构，3x3为卷积核\n",
    "    该结构中最后的relu层在short-cut连接之后.\n",
    "    整个网络分为三个stages，每个stage前面的特征图(feature map)需要降采样(downsampled),\n",
    "    通过一个步长(strides)为2的卷积层实现。\n",
    "    每个阶段(stage)的卷积核(filter)数目都会加倍。\n",
    "    相同阶段内保持相同的卷积核数目和卷积特征图大小。\n",
    "    各个阶段的特征图大小为:\n",
    "    stage 0:32x32, 16\n",
    "    stage 1:16x16, 32\n",
    "    stage 2:8x8,   64\n",
    "    模型大小如下：\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "    \n",
    "    # 输入参数\n",
    "        input_shape (tensor):输入图片tensor的尺寸(shape)\n",
    "        depth(int):核心卷积层数目\n",
    "        num_classed(int):图像分类类别(CIFAR10 有10类)\n",
    "        \n",
    "    # 返回值\n",
    "        model (Model):keras的模型实例\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # 开始实现模型的定义\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # 残差单元(residual units)堆栈(stack)的实例实现\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # 每个stack的首层但是不是第一stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # 每个stack的首层但是不是第一stack\n",
    "                # 因为进行了特征图的降采样，所以需要在shortcut连接(shortcut connection)过程中,'\n",
    "                # 进行线性投影(linear projection)以匹配降采样后的维度\n",
    "                # 通过strides变换的卷积网络实现\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # 在上层添加分类器\n",
    "    # v1版本没有在最后的shortcut connection-Relu后添加BN层\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x) # Flatten层用来将输入压平，常用于从卷积层到全连接层的过渡\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y) #he正态分布初始化\n",
    "\n",
    "    # 实例化模型\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"\n",
    "    ResNet V2 模型构建\n",
    "    网络最基本的堆栈组合为:(1X1)-(3x3)-(1x1)BN-RELU-Conv2D,称之为bottleneck层结构\n",
    "    每个stage(stack)第一层shortcut connection层需要接1x1 Conv2D卷积层，进行尺寸转换\n",
    "    接下来的第二层以及之后层则直连完成shortcut connection\n",
    "    每个stage的开始阶段，特征图都需要先进行降采样(downsampled),\n",
    "    通过strides=2的卷积层实现，filters数目需要翻倍。\n",
    "    每个stage内，每层都拥有相同数目的滤波器数目和特征图大小。\n",
    "    特征图大小如下：\n",
    "    conv1 : 32x32, 16\n",
    "    stage 0:32x32, 64\n",
    "    stage 1:16x16, 128\n",
    "    stage 2:8x8,   256\n",
    "    \n",
    "    # 输入参数\n",
    "        input_shape(tensor):输入图片tensor的尺寸(shape)\n",
    "        depth(int):核心卷积层的数目\n",
    "        num_class(int):类别数目(CIFAR10类别为10)\n",
    "        \n",
    "    # 返回值\n",
    "        model (Model):Keras模型实例\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # 开始模型定义\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2版本在分离输入tensor为两个路径(paths)之前需要先进行卷积(Conv2D with BN-ReLU)\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # 残差单元堆栈实例化\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # 第一个stage的第一层\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # 第一层，但非第一 stage\n",
    "                    strides = 2    # 需要降采样\n",
    "\n",
    "            # bottleneck残差单元\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # 卷积操作进行shortcut connection的线性映射以匹配变化了的维度\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # 在上层添加分类器网络\n",
    "    # v2版本在池化(Pooling)前需要添加BN-ReLU层\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # 实例化模型\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 32, 32, 16)   448         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 16)   64          conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 32, 32, 16)   0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 32, 32, 16)   2320        activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 32, 32, 16)   64          conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 32, 32, 16)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 32, 32, 16)   2320        activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 32, 32, 16)   64          conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 32, 32, 16)   0           activation_77[0][0]              \n",
      "                                                                 batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 32, 32, 16)   0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 32, 32, 16)   2320        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 32, 32, 16)   64          conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 32, 32, 16)   0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 32, 32, 16)   2320        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 32, 32, 16)   64          conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 32, 32, 16)   0           activation_79[0][0]              \n",
      "                                                                 batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 32, 32, 16)   0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 32, 32, 16)   2320        activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 32, 32, 16)   64          conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 32, 32, 16)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 32, 32, 16)   2320        activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 32, 32, 16)   64          conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 32, 32, 16)   0           activation_81[0][0]              \n",
      "                                                                 batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 32, 32, 16)   0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 16, 16, 32)   4640        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 16, 16, 32)   128         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 16, 16, 32)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 16, 16, 32)   9248        activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 16, 16, 32)   544         activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 32)   128         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 16, 16, 32)   0           conv2d_94[0][0]                  \n",
      "                                                                 batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 16, 16, 32)   0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 16, 16, 32)   9248        activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 16, 16, 32)   128         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 16, 16, 32)   0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 16, 16, 32)   9248        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 16, 16, 32)   128         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 16, 16, 32)   0           activation_85[0][0]              \n",
      "                                                                 batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 16, 16, 32)   0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 16, 16, 32)   9248        activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 16, 16, 32)   128         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 16, 16, 32)   0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 16, 16, 32)   9248        activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 16, 16, 32)   128         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 16, 16, 32)   0           activation_87[0][0]              \n",
      "                                                                 batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 16, 16, 32)   0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 8, 8, 64)     18496       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 64)     256         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 64)     0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 8, 8, 64)     36928       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 8, 8, 64)     2112        activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 64)     256         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 8, 8, 64)     0           conv2d_101[0][0]                 \n",
      "                                                                 batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 64)     0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 8, 8, 64)     36928       activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 64)     256         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 64)     0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 8, 8, 64)     36928       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 64)     256         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 8, 8, 64)     0           activation_91[0][0]              \n",
      "                                                                 batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 64)     0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 8, 8, 64)     36928       activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 64)     256         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 8, 8, 64)     0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 8, 8, 64)     36928       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 8, 8, 64)     256         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 8, 8, 64)     0           activation_93[0][0]              \n",
      "                                                                 batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 8, 8, 64)     0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 1, 1, 64)     0           activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 64)           0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           650         flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v1\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 312s 100ms/step - loss: 1.5947 - acc: 0.4818 - val_loss: 1.5713 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.51640, saving model to models/cifar10_ResNet20v1_model.001.h5\n",
      "Epoch 2/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 265s 85ms/step - loss: 1.2072 - acc: 0.6370 - val_loss: 1.3945 - val_acc: 0.6066\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.51640 to 0.60660, saving model to models/cifar10_ResNet20v1_model.002.h5\n",
      "Epoch 3/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 276s 88ms/step - loss: 1.0676 - acc: 0.6917 - val_loss: 0.9612 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.60660 to 0.72680, saving model to models/cifar10_ResNet20v1_model.003.h5\n",
      "Epoch 4/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 261s 84ms/step - loss: 0.9786 - acc: 0.7284 - val_loss: 1.1044 - val_acc: 0.6874\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.72680\n",
      "Epoch 5/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 261s 84ms/step - loss: 0.9215 - acc: 0.7491 - val_loss: 0.9581 - val_acc: 0.7372\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.72680 to 0.73720, saving model to models/cifar10_ResNet20v1_model.005.h5\n",
      "Epoch 6/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 261s 84ms/step - loss: 0.8801 - acc: 0.7683 - val_loss: 0.8495 - val_acc: 0.7835\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.73720 to 0.78350, saving model to models/cifar10_ResNet20v1_model.006.h5\n",
      "Epoch 7/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 260s 83ms/step - loss: 0.8514 - acc: 0.7766 - val_loss: 1.1488 - val_acc: 0.6938\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.78350\n",
      "Epoch 8/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 260s 83ms/step - loss: 0.8275 - acc: 0.7875 - val_loss: 0.8583 - val_acc: 0.7840\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.78350 to 0.78400, saving model to models/cifar10_ResNet20v1_model.008.h5\n",
      "Epoch 9/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 260s 83ms/step - loss: 0.8053 - acc: 0.7958 - val_loss: 0.9273 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.78400\n",
      "Epoch 10/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 260s 83ms/step - loss: 0.7858 - acc: 0.8045 - val_loss: 0.8533 - val_acc: 0.7881\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.78400 to 0.78810, saving model to models/cifar10_ResNet20v1_model.010.h5\n",
      "Epoch 11/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 261s 83ms/step - loss: 0.7727 - acc: 0.8092 - val_loss: 1.0937 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.78810\n",
      "Epoch 12/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 260s 83ms/step - loss: 0.7609 - acc: 0.8139 - val_loss: 0.7869 - val_acc: 0.8089\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.78810 to 0.80890, saving model to models/cifar10_ResNet20v1_model.012.h5\n",
      "Epoch 13/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 259s 83ms/step - loss: 0.7482 - acc: 0.8195 - val_loss: 0.8320 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80890\n",
      "Epoch 14/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 260s 83ms/step - loss: 0.7348 - acc: 0.8249 - val_loss: 0.7548 - val_acc: 0.8166\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.80890 to 0.81660, saving model to models/cifar10_ResNet20v1_model.014.h5\n",
      "Epoch 15/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 260s 83ms/step - loss: 0.7285 - acc: 0.8258 - val_loss: 1.0691 - val_acc: 0.7290\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.81660\n",
      "Epoch 16/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 264s 84ms/step - loss: 0.7197 - acc: 0.8296 - val_loss: 0.8380 - val_acc: 0.7988\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.81660\n",
      "Epoch 17/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 263s 84ms/step - loss: 0.7089 - acc: 0.8344 - val_loss: 0.8024 - val_acc: 0.8096\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.81660\n",
      "Epoch 18/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 348s 111ms/step - loss: 0.7017 - acc: 0.8357 - val_loss: 0.7371 - val_acc: 0.8262\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.81660 to 0.82620, saving model to models/cifar10_ResNet20v1_model.018.h5\n",
      "Epoch 19/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 300s 96ms/step - loss: 0.6954 - acc: 0.8394 - val_loss: 0.7666 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.82620 to 0.82670, saving model to models/cifar10_ResNet20v1_model.019.h5\n",
      "Epoch 20/20\n",
      "Learning rate:  0.001\n",
      "3125/3125 [==============================] - 276s 88ms/step - loss: 0.6929 - acc: 0.8393 - val_loss: 0.8844 - val_acc: 0.7866\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.82670\n",
      "10000/10000 [==============================] - 13s 1ms/step\n",
      "Test loss: 0.8843909492492675\n",
      "Test accuracy: 0.7866\n"
     ]
    }
   ],
   "source": [
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model_type)\n",
    "\n",
    "# 设置模型存储路径\n",
    "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "save_dir = 'models/'\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# 设置callbacks函数进行回调保存模型和调整学习率\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "# 模型训练,有(无)数据增强\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # 进行图片预处理与实时的数据增强：\n",
    "    datagen = ImageDataGenerator(\n",
    "        # 控制输入数据集的均值为0\n",
    "        featurewise_center=False,\n",
    "        # 控制单个样本均值为0\n",
    "        samplewise_center=False,\n",
    "        # 全部输入是否除以数据集的标准偏差(std)\n",
    "        featurewise_std_normalization=False,\n",
    "        # 每个输入是否除以数据集的标准偏差\n",
    "        samplewise_std_normalization=False,\n",
    "        # 是否应用ZCA白化\n",
    "        zca_whitening=False,\n",
    "        # ZCA白化的最小表示值(epsilon)\n",
    "        zca_epsilon=1e-06,\n",
    "        # 随机旋转(度数,0-180°)\n",
    "        rotation_range=0,\n",
    "        # 随机水平平移(整个宽度比例)\n",
    "        width_shift_range=0.1,\n",
    "        # 随机竖直平移(整个高度比例)\n",
    "        height_shift_range=0.1,\n",
    "        # 设置随机仿射(shear)变换范围\n",
    "        shear_range=0.,\n",
    "        # 设置随机放大缩小范围(zoom)\n",
    "        zoom_range=0.,\n",
    "        # 设置随机通道偏移(channel shift)范围\n",
    "        channel_shift_range=0.,\n",
    "        # 输入边界外部填充点模式设置\n",
    "        fill_mode='nearest',\n",
    "        # 对于fill_mode=\"constant\"的value参数设置\n",
    "        cval=0.,\n",
    "        # 是否随机翻转(水平)\n",
    "        horizontal_flip=True,\n",
    "        # 是否随机翻转(竖直)\n",
    "        vertical_flip=False,\n",
    "        # 设置缩放因子(在应用其他所有变换之前都要设置)\n",
    "        rescale=None,\n",
    "        # 设置对每个输入应用的预处理函数\n",
    "        preprocessing_function=None,\n",
    "        # 图片数据格式，\"channels_first\"或者\"channels_last\"\n",
    "        data_format=None,\n",
    "        # 用于验证的图片数目组成分数(0-1之间)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # 生成训练增强数据\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # fit训练\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size, # 可能是版本的问题，需要提供该参数，数据集轮数\n",
    "                        epochs=epochs, verbose=1, workers=4,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "# 评价训练得到的模型\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自建数据网络训练(Sequential式)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用自己下载设计的数据进行简单网络的训练，自己下载的数据包含5类:bus,dinosaur,elephant,flower,horse\n",
    "\n",
    "数据集一共有500张图片，每类各100张，其中验证集20张，训练集80张"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (400, 150, 150, 3)\n",
      "400 train samples\n",
      "100 test samples\n",
      "y_train shape: (400, 5)\n",
      "Epoch 1/20\n",
      "   7/3125 [..............................] - ETA: 7:37:23 - loss: 2.7160 - acc: 0.1964"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# GPU性能不够，禁用GPU，用CPU来跑\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "batch_size = 16\n",
    "num_classes = 5 # 五类\n",
    "epochs = 20\n",
    "data_augmentation = True\n",
    "# num_predictions = 20\n",
    "\n",
    "# 分割加载数据\n",
    "image_dir = 'pics/class_5/all'\n",
    "class_name = ('bus', 'dinasour', 'elephant', 'flower', 'horse')\n",
    "# 读取在all里面有几个文件夹，即为几个类别\n",
    "text = os.listdir(image_dir)\n",
    "def read_image(imageName):\n",
    "    #im = Image.open(imageName).convert('L')\n",
    "    img = image.load_img(imageName, target_size=(150, 150))\n",
    "    data = image.img_to_array(img)\n",
    "    return data\n",
    "# 把文件夹里面的图片和对应文件夹名字对应得到类别信息(label)\n",
    "images = []\n",
    "labels = []\n",
    "for textPath in text:\n",
    "    for fn in os.listdir(os.path.join(image_dir, textPath)):\n",
    "        if fn.endswith('.jpg'):\n",
    "            fd = os.path.join(image_dir, textPath, fn)\n",
    "            images.append(read_image(fd))\n",
    "            labels.append(textPath)\n",
    "# 将images和labels变成numpy数组类型\n",
    "X = np.array(images)\n",
    "y = np.array(list(map(int, labels)))\n",
    "# 进行训练集和测试集的拆分\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=30)\n",
    "# 多分类标签生成\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('x_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# 网络结构配置\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(150,150,3))) #输入size可调\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# 训练参数设置\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "# 生成训练数据\n",
    "# 进行数据预处理和实时的数据增强:\n",
    "print('Using real-time data augmentation.')\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # 像素点转数字0-1，张量数字除以255\n",
    "    rotation_range=40,  # 随机旋转(度数,0-180°)\n",
    "    horizontal_flip=True,  # 随机水平翻转\n",
    "    shear_range=0.2, # 剪切强度\n",
    "    zoom_range=0.2, # 随机缩放的幅度\n",
    "    vertical_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) # 验证集数据无需增强  \n",
    "\n",
    "# 生成训练增强数据\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'pics/class_5/train', \n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')                               # matt，多分类\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'pics/class_5/test',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')                             # matt，多分类\n",
    "\n",
    "# fit训练\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=train_generator.__iter__().n/batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_generator.__iter__().n/batch_size)\n",
    "                \n",
    "model.save_weights('models/zheng_modified_class_5.h5')  \n",
    "\"\"\"\n",
    "\n",
    "# 生成训练数据\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# 生成训练增强数据\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,  # 随机旋转(度数,0-180°)\n",
    "    horizontal_flip=True,  # 随机水平翻转\n",
    "    shear_range=0.2, # 剪切强度\n",
    "    zoom_range=0.2, # 随机缩放的幅度\n",
    "    vertical_flip=True)\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "# 设置callbacks函数进行回调保存模型和调整学习率\n",
    "filepath = 'models/zheng_modified_class_5.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer]\n",
    "\n",
    "# fit训练\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(train_datagen.flow(X_train, y_train,batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size, # 可能是版本的问题，需要提供该参数，数据集轮数\n",
    "                    epochs=epochs, verbose=1, workers=4,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (400,)\n",
      "400 train samples\n",
      "100 test samples\n",
      "y_train shape: (400,)\n",
      "preds: [[1.4979191e-05 7.9999238e-01 1.9997072e-01 5.3960689e-06 1.6465221e-05]]\n",
      "Prdicted type: dinasour\n",
      "Score: 0.7999924\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# GPU性能不够，禁用GPU，用CPU来跑\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# 加载模型(模型需要定义好结构)\n",
    "# 网络结构配置\n",
    "num_classes = 5\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(150,150,3))) #输入size可调\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.load_weights('models/zheng_try_class_5.h5')\n",
    "\"\"\"\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "# 评价模型(理论上测试数据应该是独立，这里仅作示例)\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\"\"\"\n",
    "# 模型预测输出\n",
    "img_path = 'pics/class_5/dinasour.jpeg'\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_x = image.img_to_array(img)\n",
    "# axis=0中添加数据,扩展数组\n",
    "img_x = np.expand_dims(img_x/255, axis=0)\n",
    "\n",
    "preds = model.predict(img_x)\n",
    "# 将结果解码为元组列表 (class, description, probability)\n",
    "# (一个列表代表批次中的一个样本）\n",
    "print ('preds:', preds)\n",
    "print ('Prdicted type:',class_name[np.argmax(preds, axis=1)[0]])\n",
    "print ('Score:',np.max(preds, axis=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 卷积层可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以VGG16为例进行卷积层的可视化，详细实现原理可参考[机器之心-卷积特征可视化](https://www.jiqizhixin.com/articles/2019-01-31-13)\n",
    "\n",
    "简而言之，卷积层可视化实际是对每层的卷积核的最大响应的图像进行生成。输入为随机噪声的灰度图，通过定义指定层的损失函数（将特征图激活后的均值作为损失函数），通过梯度上升的方式进行最大化激活值。最终反向生成能够得到相应卷积核最大激活输出的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Compute filters 0 to 512\n",
      "Costs of filter   0:   688 ( 4.87s )\n",
      "Costs of filter   1:  1440 ( 5.90s )\n",
      "Costs of filter   3:   761 ( 4.95s )\n",
      "Costs of filter   4:  1282 ( 5.58s )\n",
      "Costs of filter   5:   454 ( 5.47s )\n",
      "Costs of filter   7:  1144 ( 6.69s )\n",
      "Costs of filter   9:   506 ( 5.25s )\n",
      "Costs of filter  14:   738 ( 5.84s )\n",
      "Costs of filter  15:   703 ( 5.28s )\n",
      "Costs of filter  16:   729 ( 5.04s )\n",
      "Costs of filter  17:  1070 ( 4.97s )\n",
      "Costs of filter  20:   616 ( 5.01s )\n",
      "Costs of filter  21:   870 ( 5.09s )\n",
      "Costs of filter  22:   869 ( 5.03s )\n",
      "Costs of filter  24:   493 ( 5.04s )\n",
      "Costs of filter  25:   765 ( 5.02s )\n",
      "Costs of filter  27:  1162 ( 5.60s )\n",
      "Costs of filter  29:   349 ( 5.23s )\n",
      "Costs of filter  30:   742 ( 5.72s )\n",
      "Costs of filter  36:   622 ( 5.38s )\n",
      "Costs of filter  39:   910 ( 5.18s )\n",
      "Costs of filter  40:   572 ( 5.27s )\n",
      "Costs of filter  42:   587 ( 5.29s )\n",
      "Costs of filter  44:  1016 ( 5.32s )\n",
      "Costs of filter  46:   830 ( 5.80s )\n",
      "Costs of filter  49:   698 ( 6.77s )\n",
      "Costs of filter  52:  1301 ( 6.34s )\n",
      "Costs of filter  53:   800 ( 5.64s )\n",
      "Costs of filter  54:  1371 ( 5.88s )\n",
      "Costs of filter  58:   507 ( 5.65s )\n",
      "Costs of filter  61:   723 ( 6.62s )\n",
      "Costs of filter  64:   974 ( 8.11s )\n",
      "Costs of filter  65:   707 ( 6.10s )\n",
      "Costs of filter  66:   521 ( 5.80s )\n",
      "Costs of filter  68:   931 ( 5.46s )\n",
      "Costs of filter  70:   774 ( 5.43s )\n",
      "Costs of filter  72:   913 ( 5.42s )\n",
      "Costs of filter  74:  1274 ( 5.43s )\n",
      "Costs of filter  75:   535 ( 5.46s )\n",
      "Costs of filter  79:   547 ( 5.52s )\n",
      "Costs of filter  82:   514 ( 5.79s )\n",
      "Costs of filter  84:   463 ( 5.57s )\n",
      "Costs of filter  85:   720 ( 5.54s )\n",
      "Costs of filter  87:   738 ( 6.31s )\n",
      "Costs of filter  88:   627 ( 6.45s )\n",
      "Costs of filter  89:   840 ( 6.18s )\n",
      "Costs of filter  90:  1148 ( 5.63s )\n",
      "Costs of filter  91:  1227 ( 5.75s )\n",
      "Costs of filter  94:   816 ( 5.67s )\n",
      "Costs of filter 103:   673 ( 5.75s )\n",
      "Costs of filter 109:   955 ( 5.83s )\n",
      "Costs of filter 110:   523 ( 5.86s )\n",
      "Costs of filter 111:   784 ( 5.84s )\n",
      "Costs of filter 112:   598 ( 5.86s )\n",
      "Costs of filter 114:   954 ( 5.89s )\n",
      "Costs of filter 116:   552 ( 6.00s )\n",
      "Costs of filter 117:  1761 ( 5.89s )\n",
      "Costs of filter 122:   744 ( 5.98s )\n",
      "Costs of filter 126:   927 ( 6.01s )\n",
      "Costs of filter 127:   887 ( 6.04s )\n",
      "Costs of filter 128:   793 ( 6.50s )\n",
      "Costs of filter 131:   447 ( 6.98s )\n",
      "Costs of filter 133:   447 ( 7.13s )\n",
      "Costs of filter 134:   700 ( 6.83s )\n",
      "Costs of filter 136:   764 ( 6.80s )\n",
      "Costs of filter 139:   595 ( 6.19s )\n",
      "Costs of filter 140:   461 ( 6.22s )\n",
      "Costs of filter 143:   810 ( 6.25s )\n",
      "Costs of filter 145:   727 ( 6.24s )\n",
      "Costs of filter 146:   849 ( 6.26s )\n",
      "Costs of filter 147:   774 ( 6.29s )\n",
      "Costs of filter 149:   338 ( 6.39s )\n",
      "Costs of filter 156:   911 ( 6.38s )\n",
      "Costs of filter 157:   976 ( 6.36s )\n",
      "Costs of filter 158:   991 ( 6.37s )\n",
      "Costs of filter 162:   792 ( 6.39s )\n",
      "Costs of filter 165:   752 ( 6.42s )\n",
      "Costs of filter 166:   838 ( 6.50s )\n",
      "Costs of filter 168:   557 ( 6.49s )\n",
      "Costs of filter 170:  1001 ( 6.51s )\n",
      "Costs of filter 171:   653 ( 6.54s )\n",
      "Costs of filter 172:  1060 ( 6.54s )\n",
      "Costs of filter 174:   692 ( 6.56s )\n",
      "Costs of filter 176:  1488 ( 6.65s )\n",
      "Costs of filter 177:   473 ( 6.66s )\n",
      "Costs of filter 179:   834 ( 6.83s )\n",
      "Costs of filter 181:   946 ( 6.60s )\n",
      "Costs of filter 182:   982 ( 6.66s )\n",
      "Costs of filter 187:   947 ( 6.68s )\n",
      "Costs of filter 194:   734 ( 6.83s )\n",
      "Costs of filter 195:   612 ( 6.84s )\n",
      "Costs of filter 196:   766 ( 6.87s )\n",
      "Costs of filter 201:   426 ( 6.93s )\n",
      "Costs of filter 203:   640 ( 6.94s )\n",
      "Costs of filter 205:  1639 ( 6.97s )\n",
      "Costs of filter 206:   356 ( 7.01s )\n",
      "Costs of filter 208:  1142 ( 6.93s )\n",
      "Costs of filter 213:   721 ( 7.06s )\n",
      "Costs of filter 218:   647 ( 7.04s )\n",
      "Costs of filter 219:   660 ( 7.17s )\n",
      "Costs of filter 220:  1653 ( 7.15s )\n",
      "Costs of filter 223:   420 ( 7.14s )\n",
      "Costs of filter 228:   725 ( 7.29s )\n",
      "Costs of filter 236:   528 ( 7.38s )\n",
      "Costs of filter 240:   813 ( 7.34s )\n",
      "Costs of filter 242:   942 ( 7.94s )\n",
      "Costs of filter 243:   914 ( 8.18s )\n",
      "Costs of filter 245:   828 ( 7.40s )\n",
      "Costs of filter 246:   362 ( 7.54s )\n",
      "Costs of filter 247:  1444 ( 7.50s )\n",
      "Costs of filter 251:   973 ( 7.50s )\n",
      "Costs of filter 253:   927 ( 7.56s )\n",
      "Costs of filter 255:   809 ( 7.54s )\n",
      "Costs of filter 257:  1059 ( 7.64s )\n",
      "Costs of filter 258:   680 ( 7.68s )\n",
      "Costs of filter 259:   519 ( 7.59s )\n",
      "Costs of filter 260:   462 ( 7.61s )\n",
      "Costs of filter 266:  1327 ( 7.67s )\n",
      "Costs of filter 268:   603 ( 7.95s )\n",
      "Costs of filter 269:   588 ( 7.84s )\n",
      "Costs of filter 270:  1116 ( 7.84s )\n",
      "Costs of filter 271:   705 ( 7.77s )\n",
      "Costs of filter 272:   598 ( 7.80s )\n",
      "Costs of filter 281:   766 ( 7.90s )\n",
      "Costs of filter 283:   546 ( 7.92s )\n",
      "Costs of filter 285:   727 ( 7.96s )\n",
      "Costs of filter 287:   533 ( 8.12s )\n",
      "Costs of filter 288:  1529 ( 8.11s )\n",
      "Costs of filter 289:   910 ( 8.14s )\n",
      "Costs of filter 292:   461 ( 8.08s )\n",
      "Costs of filter 293:   720 ( 8.20s )\n",
      "Costs of filter 295:   375 ( 8.40s )\n",
      "Costs of filter 297:   717 ( 8.27s )\n",
      "Costs of filter 300:   862 ( 8.18s )\n",
      "Costs of filter 303:   517 ( 8.17s )\n",
      "Costs of filter 306:  1385 ( 8.40s )\n",
      "Costs of filter 308:  1145 ( 8.40s )\n",
      "Costs of filter 310:   670 ( 8.34s )\n",
      "Costs of filter 311:   634 ( 8.34s )\n",
      "Costs of filter 316:   534 ( 8.40s )\n",
      "Costs of filter 319:   916 ( 8.42s )\n",
      "Costs of filter 320:   920 ( 8.48s )\n",
      "Costs of filter 322:   521 ( 8.60s )\n",
      "Costs of filter 323:   553 ( 8.57s )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costs of filter 327:   558 ( 8.55s )\n",
      "Costs of filter 328:  1131 ( 8.70s )\n",
      "Costs of filter 329:   607 ( 8.70s )\n",
      "Costs of filter 331:   689 ( 8.73s )\n",
      "Costs of filter 334:  1178 ( 8.64s )\n",
      "Costs of filter 335:   652 ( 8.67s )\n",
      "Costs of filter 339:   694 ( 9.14s )\n",
      "Costs of filter 341:   650 ( 8.90s )\n",
      "Costs of filter 346:   527 ( 8.83s )\n",
      "Costs of filter 347:   719 ( 9.00s )\n",
      "Costs of filter 348:   547 ( 8.91s )\n",
      "Costs of filter 350:   560 ( 8.92s )\n",
      "Costs of filter 352:   584 ( 9.07s )\n",
      "Costs of filter 353:   541 ( 9.07s )\n",
      "Costs of filter 354:   576 ( 9.12s )\n",
      "Costs of filter 355:  1128 ( 9.13s )\n",
      "Costs of filter 356:  1252 ( 9.18s )\n",
      "Costs of filter 357:   423 ( 9.29s )\n",
      "Costs of filter 359:   764 ( 9.16s )\n",
      "Costs of filter 360:   445 ( 9.10s )\n",
      "Costs of filter 365:   525 ( 9.13s )\n",
      "Costs of filter 368:   931 ( 9.18s )\n",
      "Costs of filter 369:  1029 ( 9.20s )\n",
      "Costs of filter 371:  1283 ( 9.39s )\n",
      "Costs of filter 375:   960 ( 9.44s )\n",
      "Costs of filter 378:   567 ( 9.36s )\n",
      "Costs of filter 383:   832 ( 9.42s )\n",
      "Costs of filter 387:   922 ( 9.63s )\n",
      "Costs of filter 390:   699 ( 9.76s )\n",
      "Costs of filter 391:  1119 ( 9.57s )\n",
      "Costs of filter 394:  1023 ( 9.63s )\n",
      "Costs of filter 395:   689 ( 9.81s )\n",
      "Costs of filter 397:   555 ( 9.69s )\n",
      "Costs of filter 399:   570 ( 9.68s )\n",
      "Costs of filter 405:   822 ( 9.91s )\n",
      "Costs of filter 406:   585 ( 10.06s )\n",
      "Costs of filter 407:   914 ( 10.02s )\n",
      "Costs of filter 410:  1100 ( 10.09s )\n",
      "Costs of filter 411:   443 ( 10.06s )\n",
      "Costs of filter 418:  1082 ( 10.06s )\n",
      "Costs of filter 421:   524 ( 10.33s )\n",
      "Costs of filter 422:   892 ( 10.12s )\n",
      "Costs of filter 424:   353 ( 10.17s )\n",
      "Costs of filter 427:   977 ( 10.31s )\n",
      "Costs of filter 428:  1256 ( 10.49s )\n",
      "Costs of filter 431:   970 ( 10.43s )\n",
      "Costs of filter 432:  1419 ( 10.59s )\n",
      "Costs of filter 434:   600 ( 10.65s )\n",
      "Costs of filter 435:   647 ( 10.37s )\n",
      "Costs of filter 436:  1008 ( 10.40s )\n",
      "Costs of filter 437:   516 ( 10.39s )\n",
      "Costs of filter 438:   591 ( 10.63s )\n",
      "Costs of filter 442:   746 ( 10.46s )\n",
      "Costs of filter 446:   722 ( 10.58s )\n",
      "Costs of filter 447:  1298 ( 10.90s )\n",
      "Costs of filter 449:   389 ( 10.79s )\n",
      "Costs of filter 450:   653 ( 10.84s )\n",
      "Costs of filter 452:   527 ( 10.69s )\n",
      "Costs of filter 453:   617 ( 10.90s )\n",
      "Costs of filter 458:   593 ( 11.02s )\n",
      "Costs of filter 459:   912 ( 10.83s )\n",
      "Costs of filter 461:   302 ( 10.85s )\n",
      "Costs of filter 464:   450 ( 10.97s )\n",
      "Costs of filter 465:   652 ( 11.16s )\n",
      "Costs of filter 467:   687 ( 10.99s )\n",
      "Costs of filter 468:   467 ( 11.07s )\n",
      "Costs of filter 469:   996 ( 11.08s )\n",
      "Costs of filter 471:   840 ( 11.26s )\n",
      "Costs of filter 473:   772 ( 11.33s )\n",
      "Costs of filter 474:   698 ( 11.35s )\n",
      "Costs of filter 475:   588 ( 11.12s )\n",
      "Costs of filter 476:   589 ( 11.68s )\n",
      "Costs of filter 481:   697 ( 13.18s )\n",
      "Costs of filter 482:   541 ( 15.79s )\n",
      "Costs of filter 483:   527 ( 13.27s )\n",
      "Costs of filter 485:   713 ( 12.31s )\n",
      "Costs of filter 486:  1268 ( 12.75s )\n",
      "Costs of filter 487:   748 ( 11.81s )\n",
      "Costs of filter 489:   376 ( 11.90s )\n",
      "Costs of filter 490:   662 ( 11.93s )\n",
      "Costs of filter 491:   593 ( 12.19s )\n",
      "Costs of filter 493:   606 ( 14.47s )\n",
      "Costs of filter 494:   717 ( 15.14s )\n",
      "Costs of filter 496:   594 ( 14.02s )\n",
      "Costs of filter 497:   708 ( 12.83s )\n",
      "Costs of filter 500:   745 ( 12.17s )\n",
      "Costs of filter 502:  1269 ( 12.06s )\n",
      "Costs of filter 503:   615 ( 12.31s )\n",
      "Costs of filter 505:   655 ( 12.59s )\n",
      "Costs of filter 511:   568 ( 14.09s )\n",
      "234 filter processed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image as pil_image\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras import layers\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "\n",
    "# GPU性能不够，禁用GPU，用CPU来跑\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    功能函数：标准化张量(tensor)\n",
    "\n",
    "    # 输入参数\n",
    "        x:输入张量\n",
    "    # 输出\n",
    "        输入张量的标准化输出\n",
    "    \"\"\"\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    \"\"\"\n",
    "    功能函数:将浮点数数组转化为有效的uint8类型图片\n",
    "    \n",
    "    # 输入参数\n",
    "        x:代表生成图片的Numpy向量数组\n",
    "    # 输出\n",
    "        处理过的numpy数组，能够通过imshow等方式显示的图片格式\n",
    "    \"\"\"\n",
    "    # normalize tensor: center on 0., ensure std is 0.25\n",
    "    # 标准化张量:数值中心保持为0，\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.25\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "def process_image(x, former):\n",
    "    \"\"\"\n",
    "    功能函数:将有效的uint8类型图片格式转化为浮点数数组\n",
    "    为'deprocess_image'的反向操作\n",
    "    \n",
    "    # 输入参数\n",
    "        x:numpy数值，能够通过imshow等进行显示\n",
    "        fromer: 之前的numpy-array数据.\n",
    "                需要确定之前的均值(mean)和variance(方差).\n",
    "                \n",
    "    #返回值\n",
    "        处理过的代表图片的numpy数组\n",
    "    \"\"\"\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((2, 0, 1))\n",
    "    return (x / 255 - 0.5) * 4 * former.std() + former.mean()\n",
    "\n",
    "\n",
    "# 默认size为412*412，太慢了，改成52*52\n",
    "def visualize_layer(model,\n",
    "                    layer_name,\n",
    "                    step=1.,\n",
    "                    epochs=15,\n",
    "                    upscaling_steps=9,\n",
    "                    upscaling_factor=1.2,\n",
    "                    output_dim=(52,52),\n",
    "                    filter_range=(0, None)):\n",
    "    \"\"\"\n",
    "    可视化得到模型指定层最大相关的滤波器(通过激活层的均值大小判断)\n",
    "    \n",
    "    #输入参数\n",
    "        model:包含层名称的模型\n",
    "        layer_name:需要进行可视化的层名称，需要为模型的一部分\n",
    "        step:梯度上升的步长大小(step size)\n",
    "        epochs:梯度上升的迭代轮数\n",
    "        upscaling_steps:upscaling的步长数目,upscaling用于生成图的扩大\n",
    "                        初始图像大小为(80,80)，最终图像为(412,412)\n",
    "        upscaling_factor:用于缓慢更新图像大小的参数(factor)，最终大小为输出大小\n",
    "                        　如官方80->412为412=80*(1.2)**9\n",
    "        output_dim:[img_width, img_height]输出图像尺寸\n",
    "        filter_range:Tupel类型[lower,upper]\n",
    "                     确定了需要计算的卷积核(滤波器)序号范围\n",
    "                     如果upper参数为'None'，\n",
    "                     则将最后一个卷积核序号作为上限\n",
    "    \"\"\"\n",
    "\n",
    "    def _generate_filter_image(input_img,\n",
    "                               layer_output,\n",
    "                               filter_index):\n",
    "        \"\"\"\n",
    "        生成指定卷积核的图像\n",
    "        \n",
    "        # 输入参数\n",
    "            input_img:输入图像张量\n",
    "            layer_output:输出图像张量\n",
    "            filter_index:需要进行处理的滤波器序号\n",
    "                        　需保证序号有效\n",
    "            \n",
    "        # 返回值\n",
    "            返回None，如无图片\n",
    "            如有图片，则返回tuple类型的图片数组(iamge array)以及最终的loss值\n",
    "        \"\"\"\n",
    "        s_time = time.time()\n",
    "\n",
    "        # 这里建立了一个损失函数来最大化对应层的nth卷积核的激活值\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "        else:\n",
    "            loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "        # 根据loss计算对输入图片的梯度值\n",
    "        grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "        # 正则化技巧(normalization trick):需要标准化梯度值\n",
    "        grads = normalize(grads)\n",
    "\n",
    "        # 该函数返回得到输入图片的损失值和梯度值\n",
    "        iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "        # 初始图片为随机噪声的灰度图\n",
    "        intermediate_dim = tuple(\n",
    "            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            input_img_data = np.random.random(\n",
    "                (1, 3, intermediate_dim[0], intermediate_dim[1]))\n",
    "        else:\n",
    "            input_img_data = np.random.random(\n",
    "                (1, intermediate_dim[0], intermediate_dim[1], 3))\n",
    "        input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "        # 这里需要逐步扩增到原先的大小(original size),\n",
    "        # 主要是为了防止可视化结构(visualized structure)的主导高频(domminating highj-frequency)现象\n",
    "        # 这种情况可能会在我们直接计算412维度图像的时候发生。\n",
    "        # 从低维开始更容易在每个维度有一个更好的开始点(starting point),\n",
    "        # 能有效防止局部最小值的出现(poor local minima)\n",
    "        for up in reversed(range(upscaling_steps)):\n",
    "            # 逐步进行梯度上升,如20 steps\n",
    "            for _ in range(epochs):\n",
    "                loss_value, grads_value = iterate([input_img_data])\n",
    "                input_img_data += grads_value * step　#step默认为1\n",
    "\n",
    "                # 一些卷积核可能输出为0，需要跳过\n",
    "                if loss_value <= K.epsilon():\n",
    "                    return None\n",
    "\n",
    "            # 计算扩增的维度\n",
    "            intermediate_dim = tuple(\n",
    "                int(x / (upscaling_factor ** up)) for x in output_dim)\n",
    "            # 得到扩增图像数据\n",
    "            img = deprocess_image(input_img_data[0])\n",
    "            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n",
    "                                                           pil_image.BICUBIC))\n",
    "            input_img_data = [process_image(img, input_img_data[0])]\n",
    "\n",
    "        # 解码得到输入激活图像\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        e_time = time.time()\n",
    "        print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n",
    "                                                                  loss_value,\n",
    "                                                                  e_time - s_time))\n",
    "        return img, loss_value\n",
    "\n",
    "    def _draw_filters(filters, n=None):\n",
    "        \"\"\"\n",
    "        绘制得到最佳的卷积核输入图像(激活均值最大),nxn网格形式\n",
    "        \n",
    "        # 输入参数\n",
    "            filters:一系列对应卷积核的生成图片以及损失值(loss)\n",
    "            n:网格维度\n",
    "              n为None的话，则使用最大的网格大小显示\n",
    "        \"\"\"\n",
    "        if n is None:\n",
    "            n = int(np.floor(np.sqrt(len(filters))))\n",
    "\n",
    "        # 认为用于最大损失值的卷积核的激活图是可观的.\n",
    "        # 这里只保留最大的n*n个卷积核\n",
    "        filters.sort(key=lambda x: x[1], reverse=True)\n",
    "        filters = filters[:n * n]\n",
    "\n",
    "        # 先建立一张足够大的黑色背景图\n",
    "        # 如8*8的卷积核数目,图像大小为412*412，间隔为5px \n",
    "        MARGIN = 5\n",
    "        width = n * output_dim[0] + (n - 1) * MARGIN\n",
    "        height = n * output_dim[1] + (n - 1) * MARGIN\n",
    "        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n",
    "\n",
    "        # 在图像上进行已保存得到的卷积核图像的填充\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                img, _ = filters[i * n + j]\n",
    "                width_margin = (output_dim[0] + MARGIN) * i\n",
    "                height_margin = (output_dim[1] + MARGIN) * j\n",
    "                stitched_filters[\n",
    "                    width_margin: width_margin + output_dim[0],\n",
    "                    height_margin: height_margin + output_dim[1], :] = img\n",
    "\n",
    "        # save the result to disk\n",
    "        # 将得到的图像结果存盘\n",
    "        save_img('out_pics/vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n",
    "\n",
    "    #　正式进行图片输出    \n",
    "    # 输入图片的占位符(placeholder)\n",
    "    assert len(model.inputs) == 1\n",
    "    input_img = model.inputs[0]\n",
    "\n",
    "    # 得到每个主要层(key layer)的符号输出(每个层的唯一标识名称)\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "    output_layer = layer_dict[layer_name]\n",
    "    assert isinstance(output_layer, layers.Conv2D)\n",
    "\n",
    "    # 得到需要处理的滤波器范围\n",
    "    filter_lower = filter_range[0]\n",
    "    filter_upper = (filter_range[1]\n",
    "                    if filter_range[1] is not None\n",
    "                    else len(output_layer.get_weights()[1]))\n",
    "    assert(filter_lower >= 0\n",
    "           and filter_upper <= len(output_layer.get_weights()[1])\n",
    "           and filter_upper > filter_lower)\n",
    "    print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n",
    "\n",
    "    # 迭代通过每个滤波器并得到对应的激活图\n",
    "    processed_filters = []\n",
    "    for f in range(filter_lower, filter_upper):\n",
    "        img_loss = _generate_filter_image(input_img, output_layer.output, f)\n",
    "\n",
    "        if img_loss is not None:\n",
    "            processed_filters.append(img_loss)\n",
    "\n",
    "    print('{} filter processed.'.format(len(processed_filters)))\n",
    "    # 最终绘制并保存最佳的滤波器图像到硬盘\n",
    "    _draw_filters(processed_filters)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # the name of the layer we want to visualize\n",
    "    # (see model definition at keras/applications/vgg16.py)\n",
    "    # 需要定义我们需要可视化的层的名字\n",
    "    # 模型定义可在　keras/applications/vgg16.py中得到\n",
    "    # 也可以通过  \"summary()\"函数得到\n",
    "    LAYER_NAME = 'block5_conv1'\n",
    "\n",
    "    # 通过ImageNet权重进行VGG16网络的构建\n",
    "    vgg = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "    print('Model loaded.')\n",
    "    vgg.summary()\n",
    "\n",
    "    #　调用可视化函数\n",
    "    visualize_layer(vgg, LAYER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
